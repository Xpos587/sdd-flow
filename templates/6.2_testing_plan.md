# Testing Plan Template

_Project:_ [Project Name]
_Step:_ [Implementation Step ID, e.g., 1.2]
_Date:_ [Date of Document Creation]

---

## Overview

**Purpose:** Describe what this testing plan validates. Focus on verifying functionality, integration, and UI behavior for a specific implementation step.

**Scope:** Indicate what components are covered (e.g., service classes, API endpoints, frontend components, integrations).

**Exclusions:** Performance and security tests are excluded from this plan.

---

## 1. Test Pre-requisites

**Dependencies:**

- Backend server: [e.g., FastAPI at localhost:8000]
- Frontend client: [e.g., Next.js at localhost:3000]
- Additional services (if any): [e.g., Voice service at localhost:8001]

**Environment Variables:**

- `JWT_SECRET_KEY`
- `DATABASE_URL`
- Other relevant config values

**Test Accounts:**

- Username: \[test user]
- Password: \[test password]
- Token acquisition method: POST /api/auth/login

**Test Data:**

- Ensure consistent test DB state before test runs
- Populate required tables with valid test data

**Testing suite:**

- Unit tests [pytest, jest]
- Integration tests [pytest, jest]
- E2E tests [playwright]

**Test folders location:**
All testing files are stored in the /test folder of a specific project (e.g., `frontend/test`, `backend/test`, etc). Then specific tests folders are:

- Unit tests - `test/unit`
- Integration tests - `test/integration`
- E2E tests - `test/e2e` for tests, `test/e2e/playwright-report` for reports

---

## 2. Unit Tests

### [Service/Class Name]

Each method/function:

#### UT-<service_id>.<service_name>-<testID> [Test Name]

Notes of naming:

- <service_id>.<service_name> is a unique identifier from the TDDoc document
- <testID> must be unique for a given <service_id>.<service_name>. When adding a new test, always check for existing tests and increment <testID> to ensure it is unique.

example: `UT-B1.CallsService-01 should create call record with valid metadata` test must be place in `test/unit/UT_B1_CallsService_01.py`

File with tests: `tests/unit/UT-<service_id>.<service_name>.py`

- **Objective:** [What is being validated]
- **Setup/Input:** [Preconditions, data or state]
- **Expected Result:** [Outcome of the test]

(Repeat this structure for each Service/Class/Function/Method tested.)

---

## 3. Integration Tests

### API Endpoint Testing

For each relevant endpoint:

#### API-<service_id>.<service_name>-<testID> [Action/Route]

example: `API-B2.AuthService-01 Login with valid credentials`

File with tests: `tests/integration/API-<service_id>.<service_name>.py`

- **Method:** [GET/POST/etc.]
- **Endpoint:** [URL path]
- **Input:** [Request body or query params]
- **Expected Result:** [API response, DB change, etc.]

### Frontend-to-Backend Integration

For each integration point:

#### INT-<service>-<interactionID> [Name of the integration point/interaction point]

Note: InteractionID is a unique identifier from the TDDoc document.

File with tests: `tests/integration/INT-<service>-<interactionID>.py`

- **Objective:** [What is being validated]
- **Setup/Input:** [Preconditions, data or state]
- **Expected Result:** [Outcome of the test]

**Test Cases:**

- Data is loaded from the API and displayed correctly
- Errors (401, 500, etc.) are handled gracefully
- JWT token is included and validated properly
- Pagination and filtering parameters function as expected

### Other Integration Tests

---

## 4. End-to-End (E2E) Testing

#### Scenario [E2E-<scenarioID>]: [Scenario Name]

- **Steps:**
  1. [User/login/service action]
  2. [API or UI interaction]
  3. [Result validation]
- **Expected Result:** [What must happen at each step]

Repeat for each complete user-facing flow that crosses system boundaries (e.g., login → data fetch → UI update).

---

## 5. Regression Testing

Regression tests re-run a selected set of previously defined test cases to confirm that existing features still behave correctly after recent changes.

### Re-Tested Test Cases

Example:

```markdown
| Test Case ID      | Description                          | Original Location    | Status |
| ----------------- | ------------------------------------ | -------------------- | ------ |
| UT-callservice-01 | Should create call record            | Unit Test Section    | Passed |
| API-calls-03      | POST /api/calls/{id}/end             | API Endpoint Tests   | Passed |
| E2E-dashboard-01  | Dashboard displays recent calls      | End-to-End Scenarios | Passed |
| INT-dashboard-02  | Frontend fetches paginated call data | Frontend Integration | Passed |
```

### Notes

- Focused on areas affected by the new [feature/refactor/bugfix]
- Run after completing new implementation and before deployment
- Failures here block release until resolved

---

## 6. Test Completion Criteria

**Functional:**

- All test cases in Unit, Integration, and E2E sections have passed
- Edge cases are handled correctly
- System behaves as expected with valid and invalid data

**Quality:**

- Clean and readable UI output (for frontend)
- Stable service behavior under typical user actions
- Proper error handling and messages

---

## 7. Test Summary Table

| Test Case ID | Description   | Expected Result     | Status    |
| ------------ | ------------- | ------------------- | --------- |
| TC-001       | \[Brief desc] | \[Expected outcome] | \[Passed] |
| TC-002       | ...           | ...                 | ...       |

---

## Notes

- All tests should be automated where possible
- Database state should be cleaned before and after tests
